{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelcahyawijaya/htdocs/python/onnx-tensorflow/onnx_tf/common/__init__.py:87: UserWarning: FrontendHandler.get_outputs_names is deprecated. It will be removed in future release.. Use node.outputs instead.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch.onnx\n",
    "import onnx\n",
    "import warnings\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools.import_pb_to_tensorboard import import_to_tensorboard\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    0: 'NINE',\n",
    "    1: 'ZERO',\n",
    "    2: 'SEVEN',\n",
    "    3: 'SIX',\n",
    "    4: 'ONE',\n",
    "    5: 'EIGHT',\n",
    "    6: 'FOUR',\n",
    "    7: 'THREE',\n",
    "    8: 'TWO',\n",
    "    9: 'FIVE', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ONNX_ML\"] = \"1\"\n",
    "warnings.filterwarnings('ignore') # Ignore all the warning messages in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64 at 0x127A370B8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = Image.open('test.jpg').convert('L')\n",
    "display(img) # show the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN architecture without Dropout layer\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1_1 = nn.Conv2d(1, 8, 3, padding=1)      \n",
    "        self.conv1_2 = nn.Conv2d(8, 16, 3, padding=1)      \n",
    "        self.conv1_3 = nn.Conv2d(16, 24, 3, padding=1)\n",
    "        \n",
    "        self.pool1 = nn.Conv2d(24, 32, 3, padding=1, stride=2) \n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(32, 40, 3, padding=1)      \n",
    "        self.conv2_2 = nn.Conv2d(40, 48, 3, padding=1)      \n",
    "        self.conv2_3 = nn.Conv2d(48, 56, 3, padding=1)\n",
    "\n",
    "        self.pool2 = nn.Conv2d(56, 64, 3, padding=1, stride=2)\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(64, 72, 3, padding=1)      \n",
    "        self.conv3_2 = nn.Conv2d(72, 80, 3, padding=1)      \n",
    "        self.conv3_3 = nn.Conv2d(80, 88, 3, padding=1)\n",
    "        \n",
    "        self.pool3 = nn.Conv2d(88, 96, 3, padding=1, stride=2)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(96, 128, 3, padding=1)      \n",
    "        self.conv4_2 = nn.Conv2d(128, 192, 3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(192, 256, 3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 64x64x1 => 32x32x8\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = F.relu(self.conv1_3(x))\n",
    "        x = F.relu(self.pool1(x))\n",
    "        \n",
    "        # 32x32x8 => 16x16x16\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = F.relu(self.conv2_3(x))\n",
    "        x = F.relu(self.pool2(x))\n",
    "        \n",
    "        # 16x16x16 => 8x8x32\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = F.relu(self.conv3_3(x))\n",
    "        x = F.relu(self.pool3(x))\n",
    "        \n",
    "        # 8x8x32 => 8x8x10\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv4_2(x))\n",
    "        x = F.relu(self.conv4_3(x))\n",
    "\n",
    "        x = x.view(-1, 256, 8*8).sum(dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1_1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_3): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool1): Conv2d(24, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2_1): Conv2d(32, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_2): Conv2d(40, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_3): Conv2d(48, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool2): Conv2d(56, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv3_1): Conv2d(64, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_2): Conv2d(72, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_3): Conv2d(80, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool3): Conv2d(88, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv4_1): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_2): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('./model_sl_3968.pt', map_location=lambda storage, location: storage))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-424.9249,  177.7810, -433.2360,  194.1867,  -90.1193, -370.1522,\n",
       "           195.3678, -831.3246, -339.9316,  -30.9633]],\n",
       "        grad_fn=<ThAddmmBackward>), 'FOUR')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test predict\n",
    "x = torch.from_numpy(np.expand_dims(np.expand_dims(np.asarray(img, dtype=np.float32), axis=0), axis=0))\n",
    "y = model(x)\n",
    "\n",
    "y_idx = torch.argmax(y)\n",
    "y, label_dict[int(y_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dummy tensor for defining input size\n",
    "dummy_input = torch.randn(1, 1, 64, 64)\n",
    "\n",
    "# Invoke export\n",
    "torch.onnx.export(model, dummy_input, \"sign_lang_net.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load('sign_lang_net.onnx') # Load the ONNX file\n",
    "tf_rep = prepare(model) # Import the ONNX model to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0']\n",
      "-----\n",
      "['70']\n",
      "-----\n",
      "{'1': <tf.Tensor 'Const:0' shape=(8, 1, 3, 3) dtype=float32>, '2': <tf.Tensor 'Const_1:0' shape=(8,) dtype=float32>, '3': <tf.Tensor 'Const_2:0' shape=(16, 8, 3, 3) dtype=float32>, '4': <tf.Tensor 'Const_3:0' shape=(16,) dtype=float32>, '5': <tf.Tensor 'Const_4:0' shape=(24, 16, 3, 3) dtype=float32>, '6': <tf.Tensor 'Const_5:0' shape=(24,) dtype=float32>, '7': <tf.Tensor 'Const_6:0' shape=(32, 24, 3, 3) dtype=float32>, '8': <tf.Tensor 'Const_7:0' shape=(32,) dtype=float32>, '9': <tf.Tensor 'Const_8:0' shape=(40, 32, 3, 3) dtype=float32>, '10': <tf.Tensor 'Const_9:0' shape=(40,) dtype=float32>, '11': <tf.Tensor 'Const_10:0' shape=(48, 40, 3, 3) dtype=float32>, '12': <tf.Tensor 'Const_11:0' shape=(48,) dtype=float32>, '13': <tf.Tensor 'Const_12:0' shape=(56, 48, 3, 3) dtype=float32>, '14': <tf.Tensor 'Const_13:0' shape=(56,) dtype=float32>, '15': <tf.Tensor 'Const_14:0' shape=(64, 56, 3, 3) dtype=float32>, '16': <tf.Tensor 'Const_15:0' shape=(64,) dtype=float32>, '17': <tf.Tensor 'Const_16:0' shape=(72, 64, 3, 3) dtype=float32>, '18': <tf.Tensor 'Const_17:0' shape=(72,) dtype=float32>, '19': <tf.Tensor 'Const_18:0' shape=(80, 72, 3, 3) dtype=float32>, '20': <tf.Tensor 'Const_19:0' shape=(80,) dtype=float32>, '21': <tf.Tensor 'Const_20:0' shape=(88, 80, 3, 3) dtype=float32>, '22': <tf.Tensor 'Const_21:0' shape=(88,) dtype=float32>, '23': <tf.Tensor 'Const_22:0' shape=(96, 88, 3, 3) dtype=float32>, '24': <tf.Tensor 'Const_23:0' shape=(96,) dtype=float32>, '25': <tf.Tensor 'Const_24:0' shape=(128, 96, 3, 3) dtype=float32>, '26': <tf.Tensor 'Const_25:0' shape=(128,) dtype=float32>, '27': <tf.Tensor 'Const_26:0' shape=(192, 128, 3, 3) dtype=float32>, '28': <tf.Tensor 'Const_27:0' shape=(192,) dtype=float32>, '29': <tf.Tensor 'Const_28:0' shape=(256, 192, 3, 3) dtype=float32>, '30': <tf.Tensor 'Const_29:0' shape=(256,) dtype=float32>, '31': <tf.Tensor 'Const_30:0' shape=(1024, 256) dtype=float32>, '32': <tf.Tensor 'Const_31:0' shape=(1024,) dtype=float32>, '33': <tf.Tensor 'Const_32:0' shape=(10, 1024) dtype=float32>, '34': <tf.Tensor 'Const_33:0' shape=(10,) dtype=float32>, '0': <tf.Tensor '0:0' shape=(1, 1, 64, 64) dtype=float32>, '35': <tf.Tensor 'transpose_2:0' shape=(1, 8, 64, 64) dtype=float32>, '36': <tf.Tensor 'Relu:0' shape=(1, 8, 64, 64) dtype=float32>, '37': <tf.Tensor 'transpose_5:0' shape=(1, 16, 64, 64) dtype=float32>, '38': <tf.Tensor 'Relu_1:0' shape=(1, 16, 64, 64) dtype=float32>, '39': <tf.Tensor 'transpose_8:0' shape=(1, 24, 64, 64) dtype=float32>, '40': <tf.Tensor 'Relu_2:0' shape=(1, 24, 64, 64) dtype=float32>, '41': <tf.Tensor 'transpose_11:0' shape=(1, 32, 32, 32) dtype=float32>, '42': <tf.Tensor 'Relu_3:0' shape=(1, 32, 32, 32) dtype=float32>, '43': <tf.Tensor 'transpose_14:0' shape=(1, 40, 32, 32) dtype=float32>, '44': <tf.Tensor 'Relu_4:0' shape=(1, 40, 32, 32) dtype=float32>, '45': <tf.Tensor 'transpose_17:0' shape=(1, 48, 32, 32) dtype=float32>, '46': <tf.Tensor 'Relu_5:0' shape=(1, 48, 32, 32) dtype=float32>, '47': <tf.Tensor 'transpose_20:0' shape=(1, 56, 32, 32) dtype=float32>, '48': <tf.Tensor 'Relu_6:0' shape=(1, 56, 32, 32) dtype=float32>, '49': <tf.Tensor 'transpose_23:0' shape=(1, 64, 16, 16) dtype=float32>, '50': <tf.Tensor 'Relu_7:0' shape=(1, 64, 16, 16) dtype=float32>, '51': <tf.Tensor 'transpose_26:0' shape=(1, 72, 16, 16) dtype=float32>, '52': <tf.Tensor 'Relu_8:0' shape=(1, 72, 16, 16) dtype=float32>, '53': <tf.Tensor 'transpose_29:0' shape=(1, 80, 16, 16) dtype=float32>, '54': <tf.Tensor 'Relu_9:0' shape=(1, 80, 16, 16) dtype=float32>, '55': <tf.Tensor 'transpose_32:0' shape=(1, 88, 16, 16) dtype=float32>, '56': <tf.Tensor 'Relu_10:0' shape=(1, 88, 16, 16) dtype=float32>, '57': <tf.Tensor 'transpose_35:0' shape=(1, 96, 8, 8) dtype=float32>, '58': <tf.Tensor 'Relu_11:0' shape=(1, 96, 8, 8) dtype=float32>, '59': <tf.Tensor 'transpose_38:0' shape=(1, 128, 8, 8) dtype=float32>, '60': <tf.Tensor 'Relu_12:0' shape=(1, 128, 8, 8) dtype=float32>, '61': <tf.Tensor 'transpose_41:0' shape=(1, 192, 8, 8) dtype=float32>, '62': <tf.Tensor 'Relu_13:0' shape=(1, 192, 8, 8) dtype=float32>, '63': <tf.Tensor 'transpose_44:0' shape=(1, 256, 8, 8) dtype=float32>, '64': <tf.Tensor 'Relu_14:0' shape=(1, 256, 8, 8) dtype=float32>, '65': <tf.Tensor 'Const_79:0' shape=(3,) dtype=int64>, '66': <tf.Tensor 'Reshape:0' shape=(1, 256, 64) dtype=float32>, '67': <tf.Tensor 'Sum:0' shape=(1, 256) dtype=float32>, '68': <tf.Tensor 'add_16:0' shape=(1, 1024) dtype=float32>, '69': <tf.Tensor 'Relu_15:0' shape=(1, 1024) dtype=float32>, '70': <tf.Tensor 'add_17:0' shape=(1, 10) dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "print(tf_rep.inputs) # Input nodes to the model\n",
    "print('-----')\n",
    "print(tf_rep.outputs) # Output nodes from the model\n",
    "print('-----')\n",
    "print(tf_rep.tensor_dict) # All nodes in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outputs(_0=array([[ 17.9355   , -18.464376 ,  -5.890565 , -20.917042 , -15.615153 ,\n",
       "         16.719385 ,   6.7994723,  -9.214588 , -16.688555 ,   4.5415797]],\n",
       "      dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_rep.run(np.random.randn(1,1,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Outputs(_0=array([[-424.92468 ,  177.7807  , -433.23593 ,  194.18683 ,  -90.119644,\n",
       "         -370.1523  ,  195.36794 , -831.32434 , -339.93158 ,  -30.962927]],\n",
       "       dtype=float32)), 'FOUR')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.expand_dims(np.expand_dims(np.asarray(img, dtype=np.float32), axis=0), axis=0)\n",
    "y = tf_rep.run(x)\n",
    "\n",
    "y, label_dict[np.argmax(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Imported. Visualize by running: tensorboard --logdir=sl_log\n"
     ]
    }
   ],
   "source": [
    "tf_rep.export_graph(\"sign_lang_net.pb\")\n",
    "import_to_tensorboard(\"sign_lang_net.pb\", \"sl_log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tensor = 'import/0:0'\n",
    "out_tensor = 'import/add_17:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-424.92468   177.7807   -433.23593   194.18683   -90.119644 -370.1523\n",
      "   195.36794  -831.32434  -339.93158   -30.962927]] FOUR\n"
     ]
    }
   ],
   "source": [
    "with gfile.FastGFile(\"sign_lang_net.pb\", 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    with tf.Session() as sess:\n",
    "        sess.graph.as_default()\n",
    "        tf.import_graph_def(graph_def)\n",
    "        tf.global_variables_initializer().run()\n",
    "        sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "        \n",
    "        feed_dict = dict([(in_tensor, x)])\n",
    "        y = sess.run(out_tensor, feed_dict=feed_dict)\n",
    "        \n",
    "        print(y, label_dict[np.argmax(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
